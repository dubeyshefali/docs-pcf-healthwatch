---
title: Healthwatch v2.2 Release Notes
owner: Healthwatch
---

<strong><%= modified_date %></strong>

This topic contains release notes for Healthwatch for VMware Tanzu v2.2.

For information about the risks and limitations of Healthwatch v2.2, see [Assumed Risks of Using Healthwatch v2.2](index.html#assumed-risks) and [Healthwatch
v2.2 Limitations](index.html#healthwatch-limitations) in _Healthwatch_.


## <a id='releases'></a> Releases

### <a id='2-2-2'></a> v2.2.2

**Release Date:** 5/18/2022

* **[Feature]** Healthwatch supports VMware Tanzu Application Service for VMs (TAS for VMs) v2.13 and earlier.

* **[Feature Improvement]** The default alert templates are updated to better display grouped alerts.

* **[Feature Improvement]** The **Logging and Metrics Pipeline** dashboard in the Grafana UI includes Syslog Agent metrics.

* **[Feature Improvement]** The **System At a Glance** dashboard in the Grafana UI does not show metrics for compiler VMs.

* **[Bug Fix]** The **System at a Glance** dashboard in the Grafana UI does not show duplicate **Canary URL** panels.

* **[Known Issue Fix]** The **RabbitMQ** dashboards in the Grafana UI show data for RabbitMQ on-demand instances that are configured to communicate over TLS.
For more information about this known issue, see [No Data on RabbitMQ Dashboards for RabbitMQ On-Demand Instances Using TLS](#rabbitmq-tls) below.

* **[Known Issue Fix]** The Grafana instance can load metrics data in the Grafana UI while multiple Prometheus instances update after you re-deploy a
highly-available (HA) Healthwatch installation. For more information about this known issue, see [No Data While Re-Deploying Highly-Available Healthwatch
Installations](#no-data-tsdb-update) below.

* **[Known Issue Fix]** The **Kubernetes Nodes** dashboard in the Grafana UI shows data for Kubernetes clusters that use the containerd runtime. For more
information about this known issue, see [No Data for containerd Clusters on Kubernetes Nodes Dashboard for TKGI v1.12 and Later](#no-data-nodes-tkgi-112)
below.

Healthwatch v2.2.2 uses the following open-source component versions:

| Component    | Packaged Version |
|--------------|------------------|
| Prometheus   | 2.35.0           |
| Grafana      | 8.5.2            |
| Alertmanager | 0.24.0           |
| PXC          | 0.42.0           |

### <a id='2-2-1'></a> v2.2.1

**Release Date:** 2/28/2022

* **[Feature]** Healthwatch supports VMware Tanzu Kubernetes Grid Integrated Edition (TKGI) v1.13 and earlier.

* **[Feature Improvement]** If you configured a generic OAuth provider to authenticate users who log in to the Grafana UI, you can configure a logout URL. For
more information, see [Grafana UI Logout URL](#grafana-logout-url) below.

* **[Breaking Change]** Healthwatch requires additional configuration in TKGI v1.13. For more information, see [Healthwatch v2.2.1 Requires Additional
Configuration in TKGI v1.13](#tkgi-1-13) below.

* **[Known Issue Fix]** The SVM Forwarder VM does not create recursive labels. For more information about this known issue, see [SVM Forwarder Creates
Recursive Metric Labels](#metric-name-recursion) below.

* **[Known Issue Fix]** The TKGI SLI exporter VM cleans up the service accounts it creates while running the TKGI SLI test suite. For more information about
this known issue, see [Healthwatch Exporter for TKGI Does Not Clean Up TKGI Service Accounts](#tkgi-service-accounts) below.

* **[Known Issue Fix]** The backup scripts for Prometheus VMs clean up the intermediary snapshots created by BOSH Backup and Restore (BBR). For more
information about this known issue, see [BBR Backup Snapshots Fill Disk Space on Prometheus VMs](#bbr-backups-tsdb) below.

* **[Known Issue]** The **RabbitMQ** dashboards in the Grafana UI show no data for RabbitMQ on-demand instances that are configured to communicate over TLS.
For more information about this known issue, see [No Data on RabbitMQ Dashboards for RabbitMQ On-Demand Instances Using TLS](#rabbitmq-tls) below.

* **[Known Issue]** The Grafana instance cannot load metrics data in the Grafana UI while multiple Prometheus instances update after you re-deploy an HA
Healthwatch installation. For more information about this known issue, see [No Data While Re-Deploying Highly-Available Healthwatch
Installations](#no-data-tsdb-update) below.

* **[Known Issue]** If you have TKGI v1.12 or later installed, the **Kubernetes Nodes** dashboard in the Grafana UI shows no data for Kubernetes clusters that
use the containerd runtime. For more information about this known issue, see [No Data for containerd Clusters on Kubernetes Nodes Dashboard for TKGI v1.12 and
Later](#no-data-nodes-tkgi-112) below.

Healthwatch v2.2.1 uses the following open-source component versions:

| Component    | Packaged Version |
|--------------|------------------|
| Prometheus   | 2.33.1           |
| Grafana      | 8.3.3            |
| Alertmanager | 0.23.0           |
| PXC          | 0.40.0           |

### <a id='2-2-0'></a> v2.2.0

**Release Date:** 1/25/2022

<p class='note'><strong>Note:</strong> Healthwatch v2.2.0 does not support TKGI v1.13. If you have TKGI v1.13 installed on your Ops Manager foundation,
  upgrade to Healthwatch v2.2.1.</p>

* **[Feature]** In new installations of Healthwatch, the **Routing rules** field in the **Alertmanager** configuration pane is pre-configured with a default
set of routing rules. For more information, see [Default Routing Rules Are Pre-Configured for Alertmanager](#routing-rules-auto-config) below.

* **[Feature]** Healthwatch can automatically configure a route for the Grafana user interface (UI) on Ops Manager foundations with VMware Tanzu Application
Service for VMs (TAS for VMs) installed. For more information, see [Automatic Grafana UI Route Configuration](#grafana-route-auto-config) below.

* **[Feature]** Healthwatch can automatically configure authentication with the User Account and Authentication (UAA) instances in TAS for VMs and TKGI for
the Grafana UI. For more information, see [Automatic UAA Authentication Configuration](#uaa-auth-auto-config) below.

* **[Feature]** The Grafana UI includes the **System at a Glance** dashboard. For more information, see [System at a Glance Dashboard in the Grafana
UI](#system-at-a-glance) below.

* **[Feature]** The SVM Forwarder VM emits the `probe_success` and `probe_duration_seconds` metrics into the Loggregator Firehose. For more information, see
[Two Canary Test Metrics Emitted in the Loggregator Firehose](#probe-metrics) below.

* **[Feature]** For Ops Manager v2.10.10 and later, the Prometheus instance scrapes BOSH Director metrics directly from the BOSH Director VM. For more
information, see [Prometheus Scrapes Metrics Directly from the BOSH Director VM](#bosh-director-direct-scrape) below.

* **[Feature Improvement]** Healthwatch uses Grafana v8, which requires a new open source license. For more information, see [Healthwatch Requires New Open
Source License for Grafana v8](#) below.

* **[Feature Improvement]** Healthwatch automatically runs canary tests for the Ops Manager Installation Dashboard. For more information, see [Healthwatch
Automatically Runs Canary Tests for the Ops Manager Installation Dashboard](#om-canary-test-auto-config) below.

* **[Feature Improvement]** Healthwatch automatically configures TKGI cluster discovery by default on Ops Manager foundations that have TKGI installed. For
more information, see [Automatic TKGI Cluster Discovery Configuration](#cluster-discovery-auto-config) below.

* **[Feature Improvement]** You can scale your Grafana, MySQL, and MySQL Proxy instances to `0`. For more information, see [Remove Grafana](#remove-grafana)
below.

* **[Feature Improvement]** Dashboards in the Grafana UI only show metrics for canary apps that are currently configured. For more information, see [Grafana
UI Dashboards Only Include Metrics for Current Canary Apps](#current-canary-apps) below.

* **[Feature Improvement]** The timeouts for the TAS for VMs SLI test suite are increased to five minutes. For more information, see [TAS for VMs SLI Test
Timeouts Are Increased](#tas-sli-test-timeout) below.

* **[Breaking Change]** If you use automated scripts to install and configure Healthwatch, you must update your scripts to reflect the new configuration
requirements. For more information, see [Update Automation Scripts](#scripts) below.

* **[Breaking Change]** If you configured a UAA instance on a different Ops Manager foundation as the authentication method for logging in to the Grafana UI
in Healthwatch v2.1, you must select **Generic OAuth** and configure the settings for the external UAA instance in the **Grafana Authentication** pane. For
more information, see [Authenticating with a UAA Instance on a Different Ops Manager Foundation](#breaking-uaa) below.

* **[Breaking Change]** The timer metric exporter VM is removed from Healthwatch Exporter for TAS for VMs. For more information, see [Timer Metric Exporter
VM is Removed](#pas-exporter-timer) below.

* **[Bug Fix]** The TAS for VMs SLI test for the `cf login` command displays success instead of failure upon timeout.

* **[Bug Fix]** The `SyslogAgent_LossRate_1M` super value metric (SVM) is corrected to follow the recommended calculation for TAS for VMs. For more
information, see the [TAS for VMs documentation](https://docs.pivotal.io/application-service/2-12/operating/monitoring/key-cap-scaling.html#scalable-syslog).

* **[Known Issue]** The SVM Forwarder VM creates recursive labels for certain metrics. For more information about this known issue, see [SVM Forwarder Creates
Recursive Metric Labels](#metric-name-recursion) below.

* **[Known Issue]** The TKGI SLI exporter VM does not clean up the service accounts it creates while running the TKGI SLI test suite. For more information
about this known issue, see [Healthwatch Exporter for TKGI Does Not Clean Up TKGI Service Accounts](#tkgi-service-accounts) below.

* **[Known Issue]** The backup scripts for Prometheus VMs do not clean up the intermediary snapshots created by BBR. For more information about this known
issue, see [BBR Backup Snapshots Fill Disk Space on Prometheus VMs](#bbr-backups-tsdb) below.

* **[Security Fix]** Apache Log4J dependencies are updated to v2.17.1 to address a critical CVE. For more information, see [CVE
2021-44832](https://nvd.nist.gov/vuln/detail/CVE-2021-44832) on the CVE website.

* **[Known Issue]** The **RabbitMQ** dashboards in the Grafana UI show no data for RabbitMQ on-demand instances that are configured to communicate over TLS.
For more information about this known issue, see [No Data on RabbitMQ Dashboards for RabbitMQ On-Demand Instances Using TLS](#rabbitmq-tls) below.

* **[Known Issue]** The Grafana instance cannot load metrics data in the Grafana UI while multiple Prometheus instances update after you re-deploy an HA
Healthwatch installation. For more information about this known issue, see [No Data While Re-Deploying Highly-Available Healthwatch
Installations](#no-data-tsdb-update) below.

* **[Known Issue]** If you have TKGI v1.12 or later installed, the **Kubernetes Nodes** dashboard in the Grafana UI shows no data for Kubernetes clusters that
use the containerd runtime. For more information about this known issue, see [No Data for containerd Clusters on Kubernetes Nodes Dashboard for TKGI v1.12 and
Later](#no-data-nodes-tkgi-112) below.

Healthwatch v2.2.0 uses the following open-source component versions:

| Component    | Packaged Version |
|--------------|------------------|
| Prometheus   | 2.32.1           |
| Grafana      | 8.3.3            |
| Alertmanager | 0.23.0           |
| PXC          | 0.40.0           |


## <a id='upgrade'></a> How to Upgrade

To upgrade from Healthwatch v2.1 to Healthwatch v2.2, see [Upgrading Healthwatch](upgrading-healthwatch.html).


## <a id='new-features'></a> New Features

Healthwatch v2.2 includes the following major features:

### <a id='routing-rules-auto-config'></a> Default Routing Rules Are Pre-Configured for Alertmanager

In new installations of Healthwatch, the **Routing rules** field in the **Alertmanager** pane of the Healthwatch tile is pre-configured with a default set of
routing rules. You can edit these routing rules according to the needs of your deployment.

For more information about configuring routing rules for Alertmanager, see [Configure Alerting](configuring/optional-config/alerting.html#configure) in
_Configuring Alerting_.

### <a id='grafana-route-auto-config'></a> Automatic Grafana UI Route Configuration

If your Ops Manager foundation has TAS for VMs installed, you can configure Healthwatch to automatically create a route for the Grafana UI in the **Grafana**
pane of the Healthwatch tile.

For more information about configuring a route for the Grafana UI, see [(Optional) Configure Grafana](configuring/configuring-healthwatch.html#grafana) in
_Configuring Healthwatch_.

### <a id='uaa-auth-auto-config'></a> Automatic UAA Authentication Configuration

When you select **UAA** as your Grafana UI authentication method in the **Grafana Authentication** pane of the Healthwatch tile, Healthwatch automatically
configures authentication with the UAA instances in TAS for VMs and TKGI for the Grafana UI. If you want to configure authentication with a UAA instance on a
different Ops Manager foundation, you must select **Generic OAuth** and configure it manually through the **Grafana Authentication** pane.

For more information about configuring UAA as your Grafana UI authentication method, see [(Optional) Configure Grafana
Authentication](configuring/configuring-healthwatch.html#authentication) in _Configuring Healthwatch_.

### <a id='system-at-a-glance'></a> System at a Glance Dashboard in the Grafana UI

The Grafana UI includes the **System at a Glance** dashboard. This dashboard displays an overview of metrics related to the health of your Ops Manager
foundation and the runtimes you have installed on that foundation.

For more information about the **System at a Glance** dashboard, see [Default Dashboards in the Grafana UI](using-grafana.html#dashboards) in _Using
Healthwatch Dashboards in the Grafana UI_.

### <a id='grafana-logout-url'></a> Grafana UI Logout URL

If you configured a generic OAuth provider to authenticate users who log in to the Grafana UI, you can configure a logout URL in the **Grafana
Authentication** pane of the Healthwatch tile.

For more information about configuring a logout URL for the Grafana UI, see [Configure Generic OAuth
Authentication](configuring/grafana-authentication.html#configure-generic-oauth) in _Configuring Grafana Authentication_.

### <a id='probe-metrics'></a> Two Canary Test Metrics Emitted in the Loggregator Firehose

If you deploy the SVM Forwarder VM in the Healthwatch Exporter for TAS for VMs tile, the SVM Forwarder VM emits the `probe_success` and
`probe_duration_seconds` canary test metrics into the Loggregator Firehose.

For more information about canary test metrics, see [Prometheus VM](metrics.html#tsdb) in _Healthwatch Metrics_. For more information about the SVM Forwarder
VM, see [(Optional) Configure Resources](configuring/configuring-exporter-tas.html#resource-config) in _Configuring Healthwatch Exporter for TAS for VMs_.

### <a id='bosh-director-direct-scrape'></a> Prometheus Scrapes Metrics Directly from the BOSH Director VM

For Ops Manager v2.10.10 and later, the Prometheus instance scrapes BOSH Director metrics directly from the BOSH Director VM instead of the Loggregator
Firehose. This allows the Prometheus VM to gather more types of metrics related to the health of the BOSH Director. These metrics appear in the **Director
Health** dashboard in the Grafana UI.

For more information about the BOSH Director metrics that the Prometheus instance scrapes, see [BOSH SLIs](metrics.html#bosh-sli) in _Healthwatch Metrics_.

### <a id='grafana-8'></a> Healthwatch Requires New Open Source License for Grafana v8

Healthwatch uses Grafana v8, which requires the Affero General Public License (AGPL).

For more information about the AGPL, see [GNU Affero General Public License](https://www.gnu.org/licenses/agpl-3.0.en.html) on the GNU site. For more
information about Grafana v8, see the [Grafana documentation](https://grafana.com/docs/grafana/latest/whatsnew/whats-new-in-v8-0/).

### <a id='om-canary-test-auto-config'></a> Healthwatch Automatically Runs Canary Tests for the Ops Manager Installation Dashboard

Healthwatch automatically runs canary tests for the Ops Manager Installation Dashboard.

### <a id='cluster-discovery-auto-config'></a> Automatic TKGI Cluster Discovery Configuration

Healthwatch automatically configures TKGI cluster discovery by default on Ops Manager foundations that have TKGI installed. If you do not want Healthwatch to
configure TKGI cluster discovery, you can disallow it through the **TKGI Cluster Discovery** pane in the Healthwatch tile.

For more information about TKGI cluster discovery, see [Configuring TKGI Cluster Discovery](configuring/optional-config/configuring-cluster-discovery.html).
For more information about allowing or disallowing TKGI cluster discovery, see [Configure TKGI Cluster Discovery in
Healthwatch](configuring/optional-config/configuring-cluster-discovery.html#configure-tkgi-cluster-discovery) in _Configuring TKGI Cluster Discovery_.

### <a id='remove-grafana'></a> Remove Grafana

If you do not want to use any Grafana instances in your Healthwatch deployment, you can set the number of Grafana, MySQL, and MySQL Proxy instances for your
Healthwatch deployment to `0` in the **Resource Config** pane of the Healthwatch tile.

For more information about scaling Healthwatch resources, see [Healthwatch Components and Resource Requirements](resources.html).

### <a id='current-canary-apps'></a> Grafana UI Dashboards Only Include Metrics for Current Canary Apps

Dashboards in the Grafana UI only show metrics for canary apps that are currently configured. Metrics for canary apps that are no longer used in your
Healthwatch deployment are removed from your dashboards, in order to avoid mixing outdated data with current data.

For more information about canary test metrics, see [Prometheus VM](metrics.html#tsdb) in _Healthwatch Metrics_.

### <a id='tas-sli-test-timeout'></a> TAS for VMs SLI Test Timeouts Are Increased

The timeouts for the TAS for VMs SLI test suite are increased to five minutes. This reduces the number of false positives you may see in your metrics data.

For more information about canary test metrics, see [TAS for VMs SLI Exporter VM](metrics.html#pas-sli-exporter) in _Healthwatch Metrics_.


## <a id='breaking-changes'></a> Breaking Changes

Healthwatch v2.2 includes the following breaking changes:

### <a id='scripts'></a> Update Automation Scripts

Many configuration options have been added, changed, or removed for Healthwatch v2.2. If you use automated scripts to install and configure Healthwatch, you
must update your scripts to reflect the new configuration requirements.

For more information about installing and configuring Healthwatch through platform automation, see [Installing, Configuring, and Deploying a Tile Through an
Automated Pipeline](installing/automated-pipeline.html).

### <a id='breaking-uaa'></a> Authenticating with a UAA Instance on a Different Ops Manager Foundation

If you are upgrading from Healthwatch v2.1 and configured **UAA** as your authentication method for logging in to the Grafana UI, Healthwatch v2.2 keeps
**UAA** as your configured authentication method by default. If you configured a UAA instance on a different Ops Manager foundation as the authentication
method for logging in to the Grafana UI in Healthwatch v2.1, you must select **Generic OAuth** and configure the settings for the external UAA instance in the
**Grafana Authentication** pane.

For more information about configuring a UAA instance on a different Ops Manager foundation as the authentication method for logging in to the Grafana UI, see
[Configuring Authentication with a UAA Instance on a Different Ops Manager Foundation](configuring/optional-config/configuring-external-uaa.html).

### <a id='pas-exporter-timer'></a> Timer Metric Exporter VM is Removed

The timer metric exporter VM, `pas-exporter-timer`, is removed from Healthwatch Exporter for TAS for VMs. This removes unnecessary data and uses fewer IaaS
resources.

For more information about the metrics for TAS for VMs that Healthwatch Exporter for TAS for VMs collects, see [Healthwatch Exporter for TAS for VMs Metric
Exporter VMs](metrics.html#pas-exporters) in _Healthwatch Metrics_.

### <a id='tkgi-1-13'></a> Healthwatch v2.2.1 Requires Additional Configuration in TKGI v1.13

After you install Healthwatch v2.2.1, you must configure TKGI v1.13 to send metrics for Kubernetes Controller Manager to Healthwatch.

For more information about configuring TKGI v1.13 to send metrics for Kubernetes Controller Manager to Healthwatch, see [Configure
TKGI](configuring/optional-config/configuring-cluster-discovery.html#configure-tkgi) in _Configuring TKGI Cluster Discovery_.


## <a id='known-issues'></a> Known Issues

Healthwatch v2.2 includes the following known issues:

### <a id='metric-name-recursion'></a> SVM Forwarder Creates Recursive Metric Labels

This known issue is fixed in Healtwatch v2.2.1 and later.

When the SVM Forwarder VM is deployed in Healthwatch Exporter for TAS for VMs, a change in the Prometheus server causes metrics with the `job` and
`exported_job` labels to become recursive. For example, `exported_job` becomes `exported_exported_exported_exported_job`.

To work around this issue, set the number of SVM Forwarder VM instances for your Healthwatch deployment to `0` in the **Resource Config** pane of the
Healthwatch Exporter for TAS for VMs tile. For more information about scaling Healthwatch resources, see [(Optional) Configure
Resources](configuring/configuring-exporter-tas.html#resource-config) in _Configuring Healthwatch Exporter for TAS for VMs_.

### <a id='no-data-nodes-tkgi-112'></a> No Data for containerd Clusters on Kubernetes Nodes Dashboard for TKGI v1.12 and Later

This known issue is fixed in Healthwatch v2.2.2 and later.

If you have TKGI v1.12 or later installed, the **Kubernetes Nodes** dashboard in the Grafana UI might not show data for Kubernetes clusters that use the
containerd runtime.

In TKGI v1.11 and earlier, the `name` label in Kubernetes cluster metrics start with `k8s_`. However, in TKGI v1.12 and later, new Kubernetes clusters run on
containerd instead of in Docker. As a result, in TKGI v1.12 and above the `name` label in Kubernetes cluster metrics start with a hex value instead of `k8s_`,
which the Grafana instance does not recognize.

To fix this issue, upgrade to Healthwatch v2.2.2 or later.

### <a id='no-data-nodes-tkgi-110'></a> No Data for Individual Pods on Kubernetes Nodes Dashboard for TKGI v1.10

If you are using TKGI v1.10.0 or v1.10.1, the **Kubernetes Nodes** dashboard in the Grafana UI might not show data for individual pods. This is due to a known
issue in Kubernetes v1.19.6 and earlier and Kubernetes v1.20.1 and earlier.

To fix this issue, upgrade to TKGI v1.10.2 or later. For more information about upgrading to TKGI v1.10.2 or later, see the [TKGI
documentation](https://docs.pivotal.io/tkgi/1-10/upgrading.html).

### <a id='tkgi-windows-clusters'></a> No Data on Kubernetes Nodes Dashboard for Windows Clusters

If you are using TKGI to monitor Windows clusters, the **Kubernetes Nodes** dashboard in the Grafana UI might not show data. Healthwatch does not currently
visualize node metrics for Windows clusters.

### <a id='tkgi-service-accounts'></a> Healthwatch Exporter for TKGI Does Not Clean Up TKGI Service Accounts

This known issue is fixed in Healthwatch v2.2.1 and later.

If you run SLI tests for TKGI through Healthwatch Exporter for TKGI, and you do not have an OpenID Connect (OIDC) provider for your Kubernetes clusters
configured for TKGI, the TKGI SLI exporter VM does not automatically clean up the service accounts that it creates while running the TKGI SLI test suite.

To fix this issue, either upgrade to Healthwatch v2.2.1 or configure an OIDC provider as the identity provider for your Kubernetes clusters in the TKGI tile.
This cleans up the service accounts that the TKGI SLI exporter VM creates in future TKGI SLI tests, but does not clean up existing service accounts from
previous TKGI SLI tests. For more information about configuring an OIDC provider in TKGI, see the [TKGI
documentation](https://docs.pivotal.io/tkgi/oidc-provider.html).

You may need to manually delete existing service accounts from previous TKGI SLI tests. For more information about manually deleting existing service
accounts, see [Healthwatch Exporter for TKGI Does Not Clean Up TKGI Service Accounts](troubleshooting.html#tkgi-service-accounts) in _Troubleshooting
Heathwatch_.

### <a id='bbr-backups-tsdb'></a> BBR Backup Snapshots Fill Disk Space on Prometheus VMs

This known issue is fixed in Healthwatch v2.2.1 and later.

In Healthwatch v2.2.0, the backup scripts for Prometheus VMs do not clean up the intermediary snapshots created by BBR. This results in the disk space on
Prometheus VMs filling up.

To fix this issue, either upgrade to Healthwatch v2.2.1 or manually clean up the snapshots. For more information about manually cleaning up the snapshots, see
[BBR Backup Snapshots Fill Disk Space on Prometheus VMs](troubleshooting.html#bbr-backups-tsdb) in _Troubleshooting Healthwatch_.

### <a id='rabbitmq-tls'></a> No Data on RabbitMQ Dashboards for RabbitMQ On-Demand Instances Using TLS

This known issue is fixed in Healthwatch v2.2.2 and later.

In Healthwatch v2.2.1 and earlier, the Prometheus instance does not scrape metrics from RabbitMQ on-demand instances that are configured to communicate over
TLS. As a result, the **RabbitMQ** dashboards in the Grafana UI show no data for RabbitMQ on-demand instances that are configured to use TLS.

To fix this issue, upgrade to Healthwatch v2.2.2 or later and RabbitMQ v2.0.13 or later.

### <a id='no-data-tsdb-update'></a> No Data While Re-Deploying Highly-Available Healthwatch Installations

This known issue is fixed in Healthwatch v2.2.2 and later.

In Healthwatch v2.2.1 and earlier, the Grafana instance cannot load metrics data in the Grafana UI after you re-deploy an HA Healthwatch installation with
multiple Prometheus instances. An HA Healthwatch installation is meant to allow the Grafana instance to continue loading data during re-deployment by ensuring
that the second Prometheus instance does not start updating until after the first Prometheus instance has updated and re-starts. In Healthwatch v2.2.1 and
earlier, a bug causes the second Prometheus instance to start updating before the first Prometheus instance re-starts.

To fix this issue, upgrade to Healthwatch v2.2.2 or later.
